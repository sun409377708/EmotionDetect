# ğŸ“± è¡¨æƒ…è¯†åˆ«æ¨¡å‹iOSé›†æˆæŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—å¸®åŠ©æ‚¨å°†è®­ç»ƒå¥½çš„è¡¨æƒ…è¯†åˆ«TFLiteæ¨¡å‹é›†æˆåˆ°iOSåº”ç”¨ä¸­ï¼Œå®ç°å®æ—¶è¡¨æƒ…è¯†åˆ«åŠŸèƒ½ã€‚

## ğŸ“‹ æ¨¡å‹ä¿¡æ¯

### å¯ç”¨æ¨¡å‹æ–‡ä»¶
| æ¨¡å‹æ–‡ä»¶ | å¤§å° | æ¨èåœºæ™¯ | ç‰¹ç‚¹ |
|---------|------|----------|------|
| `tflite_models/emotion_model_int8.tflite` | 0.69 MB | **ç§»åŠ¨ç«¯é¦–é€‰** | æœ€å¿«æ¨ç†é€Ÿåº¦ |
| `tflite_models/emotion_model_dynamic.tflite` | 0.68 MB | å¹³è¡¡é€‰æ‹© | ç²¾åº¦/é€Ÿåº¦å¹³è¡¡ |
| `emotion_model.tflite` | 0.68 MB | å‘åå…¼å®¹ | åŸæœ‰æ¨¡å‹ |

### æ¨¡å‹è§„æ ¼
- **è¾“å…¥**: `[1, 48, 48, 1]` - 48x48ç°åº¦å›¾åƒ
- **è¾“å‡º**: `[1, 7]` - 7ç§è¡¨æƒ…æ¦‚ç‡åˆ†å¸ƒ
- **è¡¨æƒ…ç±»åˆ«**: æ„¤æ€’ã€åŒæ¶ã€ææƒ§ã€å¼€å¿ƒã€æ‚²ä¼¤ã€æƒŠè®¶ã€ä¸­æ€§
- **å‡†ç¡®ç‡**: ~56.2% (TFLiteç‰ˆæœ¬)
- **æ¨ç†æ—¶é—´**: 0.32ms/å¼  (å‚è€ƒ)

---

## ğŸ iOSé›†æˆ

### ğŸ“¦ ä¾èµ–é…ç½®

#### Podfile
```ruby
platform :ios, '12.0'

target 'EmotionRecognitionApp' do
  use_frameworks!
  
  # TensorFlow Lite
  pod 'TensorFlowLiteSwift'
  pod 'TensorFlowLiteSelectTfOps'
  
  # å›¾åƒå¤„ç†
  pod 'OpenCV2'
  
  # UIç»„ä»¶
  pod 'SnapKit'
  
end
```

### ğŸ”§ æ ¸å¿ƒå®ç°ä»£ç 

#### EmotionClassifier.swift
```swift
import Foundation
import TensorFlowLite
import UIKit
import VideoToolbox

class EmotionClassifier {
    
    // MARK: - Constants
    private let modelName = "emotion_model_int8"
    private let inputSize = 48
    private let numClasses = 7
    
    private let emotionLabels = [
        "æ„¤æ€’", "åŒæ¶", "ææƒ§", "å¼€å¿ƒ", "æ‚²ä¼¤", "æƒŠè®¶", "ä¸­æ€§"
    ]
    
    // MARK: - Properties
    private var interpreter: Interpreter
    private let batchSize = 1
    private let inputChannels = 1
    
    // MARK: - Initialization
    init() throws {
        // åŠ è½½æ¨¡å‹æ–‡ä»¶
        guard let modelPath = Bundle.main.path(forResource: modelName, ofType: "tflite") else {
            throw ClassifierError.invalidModel("æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: \(modelName).tflite")
        }
        
        // é…ç½®è§£é‡Šå™¨é€‰é¡¹
        var options = Interpreter.Options()
        options.threadCount = 4
        
        do {
            // åˆ›å»ºè§£é‡Šå™¨
            interpreter = try Interpreter(modelPath: modelPath, options: options)
            
            // åˆ†é…å¼ é‡
            try interpreter.allocateTensors()
            
            print("EmotionClassifieråˆå§‹åŒ–æˆåŠŸ")
            
        } catch {
            throw ClassifierError.initializationFailed("è§£é‡Šå™¨åˆå§‹åŒ–å¤±è´¥: \(error)")
        }
    }
    
    // MARK: - Classification
    func classify(image: UIImage) -> EmotionResult? {
        do {
            // é¢„å¤„ç†å›¾åƒ
            guard let inputData = preprocessImage(image) else {
                print("å›¾åƒé¢„å¤„ç†å¤±è´¥")
                return nil
            }
            
            // æ¨ç†
            let startTime = Date()
            
            try interpreter.copy(inputData, toInputAt: 0)
            try interpreter.invoke()
            
            let inferenceTime = Date().timeIntervalSince(startTime) * 1000 // ms
            
            // è·å–è¾“å‡º
            let outputTensor = try interpreter.output(at: 0)
            
            // åå¤„ç†
            return postprocessOutput(outputTensor.data, inferenceTime: inferenceTime)
            
        } catch {
            print("åˆ†ç±»å¤±è´¥: \(error)")
            return nil
        }
    }
    
    // MARK: - Private Methods
    private func preprocessImage(_ image: UIImage) -> Data? {
        // 1. è°ƒæ•´å¤§å°åˆ°48x48
        guard let resizedImage = image.resized(to: CGSize(width: inputSize, height: inputSize)),
              let cgImage = resizedImage.cgImage else {
            return nil
        }
        
        // 2. è½¬æ¢ä¸ºç°åº¦å¹¶åˆ›å»ºæ•°æ®
        let width = cgImage.width
        let height = cgImage.height
        let bytesPerPixel = 4
        let bytesPerRow = width * bytesPerPixel
        let bitsPerComponent = 8
        
        var pixelData = [UInt8](repeating: 0, count: height * bytesPerRow)
        
        let colorSpace = CGColorSpaceCreateDeviceRGB()
        let context = CGContext(data: &pixelData,
                               width: width,
                               height: height,
                               bitsPerComponent: bitsPerComponent,
                               bytesPerRow: bytesPerRow,
                               space: colorSpace,
                               bitmapInfo: CGImageAlphaInfo.noneSkipLast.rawValue)
        
        context?.draw(cgImage, in: CGRect(x: 0, y: 0, width: width, height: height))
        
        // 3. è½¬æ¢ä¸ºç°åº¦å¹¶é‡åŒ–ä¸ºINT8
        var inputData = Data()
        for y in 0..<height {
            for x in 0..<width {
                let pixelIndex = y * width + x
                let r = Float(pixelData[pixelIndex * 4])
                let g = Float(pixelData[pixelIndex * 4 + 1])
                let b = Float(pixelData[pixelIndex * 4 + 2])
                
                // è½¬æ¢ä¸ºç°åº¦
                let gray = 0.299 * r + 0.587 * g + 0.114 * b
                
                // INT8é‡åŒ– (0-255 -> -128-127)
                let quantizedGray = Int8(gray) - 128
                inputData.append(Data([UInt8(bitPattern: quantizedGray)]))
            }
        }
        
        return inputData
    }
    
    private func postprocessOutput(_ outputData: Data, inferenceTime: Double) -> EmotionResult {
        // å°†è¾“å‡ºæ•°æ®è½¬æ¢ä¸ºFloatæ•°ç»„
        let probabilities = outputData.withUnsafeBytes { bytes in
            Array(bytes.bindMemory(to: Float32.self))
        }
        
        // æ‰¾åˆ°æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«
        guard let maxIndex = probabilities.indices.max(by: { probabilities[$0] < probabilities[$1] }) else {
            return EmotionResult(emotion: "æœªçŸ¥", confidence: 0.0, allProbabilities: [], inferenceTime: inferenceTime)
        }
        
        let emotion = emotionLabels[maxIndex]
        let confidence = probabilities[maxIndex]
        
        return EmotionResult(
            emotion: emotion,
            confidence: confidence,
            allProbabilities: probabilities,
            inferenceTime: inferenceTime
        )
    }
}

// MARK: - Extensions
extension UIImage {
    func resized(to size: CGSize) -> UIImage? {
        UIGraphicsBeginImageContextWithOptions(size, false, scale)
        defer { UIGraphicsEndImageContext() }
        draw(in: CGRect(origin: .zero, size: size))
        return UIGraphicsGetImageFromCurrentImageContext()
    }
}

// MARK: - Supporting Types
struct EmotionResult {
    let emotion: String
    let confidence: Float
    let allProbabilities: [Float]
    let inferenceTime: Double
    
    var description: String {
        return "è¡¨æƒ…: \(emotion), ç½®ä¿¡åº¦: \(String(format: "%.1f", confidence * 100))%, æ¨ç†æ—¶é—´: \(String(format: "%.1f", inferenceTime))ms"
    }
}

enum ClassifierError: Error, LocalizedError {
    case invalidModel(String)
    case initializationFailed(String)
    
    var errorDescription: String? {
        switch self {
        case .invalidModel(let message):
            return "æ¨¡å‹é”™è¯¯: \(message)"
        case .initializationFailed(let message):
            return "åˆå§‹åŒ–å¤±è´¥: \(message)"
        }
    }
}
```

#### ViewController.swift
```swift
import UIKit
import AVFoundation

class ViewController: UIViewController {
    
    // MARK: - IBOutlets
    @IBOutlet weak var previewView: UIView!
    @IBOutlet weak var resultLabel: UILabel!
    @IBOutlet weak var confidenceLabel: UILabel!
    
    // MARK: - Properties
    private var captureSession: AVCaptureSession!
    private var previewLayer: AVCaptureVideoPreviewLayer!
    private var emotionClassifier: EmotionClassifier!
    
    private let sessionQueue = DispatchQueue(label: "session queue")
    private let classificationQueue = DispatchQueue(label: "classification queue")
    
    // MARK: - Lifecycle
    override func viewDidLoad() {
        super.viewDidLoad()
        setupUI()
        initializeClassifier()
        setupCamera()
    }
    
    override func viewDidLayoutSubviews() {
        super.viewDidLayoutSubviews()
        previewLayer?.frame = previewView.bounds
    }
    
    // MARK: - Setup Methods
    private func setupUI() {
        resultLabel.text = "å‡†å¤‡å°±ç»ª"
        confidenceLabel.text = ""
        
        // è®¾ç½®é¢„è§ˆè§†å›¾
        previewView.backgroundColor = .black
        previewView.layer.cornerRadius = 12
    }
    
    private func initializeClassifier() {
        do {
            emotionClassifier = try EmotionClassifier()
        } catch {
            showAlert(title: "é”™è¯¯", message: "æ¨¡å‹åŠ è½½å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    private func setupCamera() {
        sessionQueue.async { [weak self] in
            self?.configureCaptureSession()
        }
    }
    
    private func configureCaptureSession() {
        captureSession = AVCaptureSession()
        captureSession.sessionPreset = .medium
        
        // æ·»åŠ è¾“å…¥è®¾å¤‡
        guard let frontCamera = AVCaptureDevice.default(.builtInWideAngleCamera,
                                                        for: .video,
                                                        position: .front),
              let input = try? AVCaptureDeviceInput(device: frontCamera) else {
            DispatchQueue.main.async {
                self.showAlert(title: "é”™è¯¯", message: "æ— æ³•è®¿é—®å‰ç½®æ‘„åƒå¤´")
            }
            return
        }
        
        if captureSession.canAddInput(input) {
            captureSession.addInput(input)
        }
        
        // æ·»åŠ è¾“å‡º
        let videoOutput = AVCaptureVideoDataOutput()
        videoOutput.setSampleBufferDelegate(self, queue: classificationQueue)
        videoOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA]
        
        if captureSession.canAddOutput(videoOutput) {
            captureSession.addOutput(videoOutput)
        }
        
        // è®¾ç½®é¢„è§ˆå±‚
        DispatchQueue.main.async {
            self.previewLayer = AVCaptureVideoPreviewLayer(session: self.captureSession)
            self.previewLayer.videoGravity = .resizeAspectFill
            self.previewLayer.frame = self.previewView.bounds
            self.previewView.layer.addSublayer(self.previewLayer)
            
            // å¼€å§‹ä¼šè¯
            self.sessionQueue.async {
                self.captureSession.startRunning()
            }
        }
    }
    
    // MARK: - Helper Methods
    private func showAlert(title: String, message: String) {
        let alert = UIAlertController(title: title, message: message, preferredStyle: .alert)
        alert.addAction(UIAlertAction(title: "ç¡®å®š", style: .default))
        present(alert, animated: true)
    }
    
    private func updateUI(with result: EmotionResult) {
        DispatchQueue.main.async {
            self.resultLabel.text = result.emotion
            self.confidenceLabel.text = "ç½®ä¿¡åº¦: \(String(format: "%.1f", result.confidence * 100))%"
        }
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate
extension ViewController: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, 
                      didOutput sampleBuffer: CMSampleBuffer, 
                      from connection: AVCaptureConnection) {
        
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        let image = UIImage(pixelBuffer: pixelBuffer)
        
        if let result = emotionClassifier.classify(image: image) {
            updateUI(with: result)
        }
    }
}

// MARK: - UIImage Extension
extension UIImage {
    convenience init?(pixelBuffer: CVPixelBuffer) {
        var cgImage: CGImage?
        VTCreateCGImageFromCVPixelBuffer(pixelBuffer, options: nil, imageOut: &cgImage)
        
        guard let cg = cgImage else { return nil }
        
        self.init(cgImage: cg)
    }
}
```

### ğŸ“± èµ„æºé…ç½®

#### Info.plist
```xml
<key>NSCameraUsageDescription</key>
<string>éœ€è¦ä½¿ç”¨ç›¸æœºè¿›è¡Œå®æ—¶è¡¨æƒ…è¯†åˆ«</string>
```

#### Bundleèµ„æº
```
YourApp.app/
â””â”€â”€ emotion_model_int8.tflite  // å°†æ¨¡å‹æ–‡ä»¶æ·»åŠ åˆ°Bundle
```

---

## âš ï¸ å…³é”®æ³¨æ„äº‹é¡¹

### ğŸ”§ æŠ€æœ¯æ³¨æ„äº‹é¡¹

#### 1. æ¨¡å‹é‡åŒ–å¤„ç†
```python
# INT8æ¨¡å‹çš„æ•°æ®é¢„å¤„ç†
def preprocess_for_int8_model(image):
    # å½’ä¸€åŒ–åˆ°0-255
    normalized = image * 255.0
    # é‡åŒ–åˆ°-128åˆ°127
    quantized = normalized.astype(np.int8) - 128
    return quantized
```

#### 2. è¾“å‡ºåé‡åŒ–
```python
# INT8æ¨¡å‹è¾“å‡ºéœ€è¦åé‡åŒ–
def dequantize_output(int8_output, scale, zero_point):
    return scale * (int8_output.astype(np.float32) - zero_point)
```

#### 3. æ€§èƒ½ä¼˜åŒ–å»ºè®®
- **å¸§ç‡æ§åˆ¶**: é™åˆ¶æ¨ç†é¢‘ç‡ï¼ˆå»ºè®®5-10 FPSï¼‰
- **çº¿ç¨‹ç®¡ç†**: åœ¨åå°çº¿ç¨‹è¿›è¡Œæ¨ç†
- **å†…å­˜ç®¡ç†**: åŠæ—¶é‡Šæ”¾ä¸ç”¨çš„å›¾åƒæ•°æ®

### ğŸ“± ç§»åŠ¨ç«¯ç‰¹æ®Šè€ƒè™‘

#### 1. è®¾å¤‡å…¼å®¹æ€§
```swift
// iOS - æ£€æŸ¥è®¾å¤‡æ€§èƒ½
func isDeviceSupported() -> Bool {
    let processInfo = ProcessInfo.processInfo
    return processInfo.physicalMemory >= 2 * 1024 * 1024 * 1024 // è‡³å°‘2GB RAM
}
```

#### 2. ç”µæ± ä¼˜åŒ–
- ä½¿ç”¨ä½åŠŸè€—æ¨¡å¼æ£€æµ‹äººè„¸
- åŠ¨æ€è°ƒæ•´æ¨ç†é¢‘ç‡
- åœ¨åå°æ—¶æš‚åœå¤„ç†

#### 3. ç”¨æˆ·ä½“éªŒ
- æ·»åŠ åŠ è½½æŒ‡ç¤ºå™¨
- æä¾›ç½®ä¿¡åº¦é˜ˆå€¼è®¾ç½®
- æ”¯æŒå‰åæ‘„åƒå¤´åˆ‡æ¢

### ğŸ”’ éšç§å®‰å…¨

#### 1. æ•°æ®å¤„ç†
- **æœ¬åœ°å¤„ç†**: æ‰€æœ‰æ¨ç†åœ¨è®¾å¤‡æœ¬åœ°è¿›è¡Œ
- **æ— æ•°æ®ä¸Šä¼ **: å›¾åƒæ•°æ®ä¸ç¦»å¼€è®¾å¤‡
- **å†…å­˜æ¸…ç†**: åŠæ—¶æ¸…ç†ä¸´æ—¶å›¾åƒæ•°æ®

#### 2. æƒé™ç®¡ç†
```swift
// iOS
import AVFoundation

func requestCameraPermission() {
    AVCaptureDevice.requestAccess(for: .video) { granted in
        DispatchQueue.main.async {
            if granted {
                self.setupCamera()
            } else {
                self.showPermissionAlert()
            }
        }
    }
}
```

### ğŸ“Š æ€§èƒ½ç›‘æ§

#### 1. æ¨ç†æ—¶é—´ç›‘æ§
```swift
// iOS
class PerformanceMonitor {
    private var inferenceTimes: [Double] = []
    
    func recordInferenceTime(_ time: Double) {
        inferenceTimes.append(time)
        if inferenceTimes.count > 100 {
            inferenceTimes.removeFirst()
        }
    }
    
    func getAverageInferenceTime() -> Double {
        return inferenceTimes.reduce(0, +) / Double(inferenceTimes.count)
    }
}
```

#### 2. å†…å­˜ä½¿ç”¨ç›‘æ§
```swift
// iOS
func getCurrentMemoryUsage() -> Double {
    var info = mach_task_basic_info()
    var count = mach_msg_type_number_t(MemoryLayout<mach_task_basic_info>.size)/4
    
    let kerr: kern_return_t = withUnsafeMutablePointer(to: &info) {
        $0.withMemoryRebound(to: integer_t.self, capacity: 1) {
            task_info(mach_task_self_,
                     task_flavor_t(MACH_TASK_BASIC_INFO),
                     $0,
                     &count)
        }
    }
    
    return kerr == KERN_SUCCESS ? Double(info.resident_size) / 1024.0 / 1024.0 : -1
}
```

### ğŸ› å¸¸è§é—®é¢˜è§£å†³

#### 1. æ¨¡å‹åŠ è½½å¤±è´¥
```swift
// æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
func checkModelFile() -> Bool {
    guard let path = Bundle.main.path(forResource: "emotion_model_int8", ofType: "tflite") else {
        print("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        return false
    }
    
    return FileManager.default.fileExists(atPath: path)
}
```

#### 2. æ¨ç†ç»“æœå¼‚å¸¸
```swift
// éªŒè¯è¾“å‡ºæ•°æ®
func validateOutput(_ probabilities: [Float]) -> Bool {
    let sum = probabilities.reduce(0, +)
    return sum > 0.99 && sum < 1.01 && probabilities.allSatisfy { $0 >= 0 }
}
```

#### 3. ç›¸æœºé¢„è§ˆé—®é¢˜
```swift
// iOS - å¤„ç†è®¾å¤‡æ—‹è½¬
override func viewWillTransition(to size: CGSize, with coordinator: UIViewControllerTransitionCoordinator) {
    super.viewWillTransition(to: size, with: coordinator)
    
    coordinator.animate(alongsideTransition: { _ in
        if let connection = self.previewLayer.connection {
            let currentDevice = UIDevice.current
            let orientation = currentDevice.orientation
            
            if let videoOrientation = self.videoOrientationFromDeviceOrientation(orientation) {
                connection.videoOrientation = videoOrientation
            }
        }
    })
}

private func videoOrientationFromDeviceOrientation(_ orientation: UIDeviceOrientation) -> AVCaptureVideoOrientation? {
    switch orientation {
    case .portrait:
        return .portrait
    case .landscapeLeft:
        return .landscapeRight
    case .landscapeRight:
        return .landscapeLeft
    case .portraitUpsideDown:
        return .portraitUpsideDown
    default:
        return nil
    }
}
```

---

## ğŸš€ é›†æˆæç¤ºè¯

### å¯¹ChatGPT/Claudeçš„æç¤ºè¯
```
æˆ‘éœ€è¦åœ¨iOSåº”ç”¨ä¸­é›†æˆä¸€ä¸ªè¡¨æƒ…è¯†åˆ«æ¨¡å‹ã€‚

æ¨¡å‹ä¿¡æ¯ï¼š
- æ¨¡å‹æ–‡ä»¶ï¼šemotion_model_int8.tflite (697KB)
- è¾“å…¥ï¼š[1, 48, 48, 1] INT8æ ¼å¼
- è¾“å‡ºï¼š[1, 7] Float32æ ¼å¼  
- 7ä¸ªç±»åˆ«ï¼šæ„¤æ€’ã€åŒæ¶ã€ææƒ§ã€å¼€å¿ƒã€æ‚²ä¼¤ã€æƒŠè®¶ã€ä¸­æ€§

éœ€æ±‚ï¼š
1. å®æ—¶ç›¸æœºé¢„è§ˆå’Œè¡¨æƒ…è¯†åˆ«
2. æ”¯æŒå‰ç½®æ‘„åƒå¤´
3. æ˜¾ç¤ºè¯†åˆ«ç»“æœå’Œç½®ä¿¡åº¦
4. è‰¯å¥½çš„ç”¨æˆ·ä½“éªŒå’Œæ€§èƒ½

è¯·æä¾›ï¼š
1. å®Œæ•´çš„Swiftä»£ç å®ç°
2. å¿…è¦çš„Podä¾èµ–é…ç½®
3. æƒé™è®¾ç½®
4. é”™è¯¯å¤„ç†æ–¹æ¡ˆ
5. æ€§èƒ½ä¼˜åŒ–å»ºè®®

å¹³å°ï¼šiOS Swift
æœ€ä½ç‰ˆæœ¬ï¼šiOS 12.0
```

### å¼€å‘æ­¥éª¤æ£€æŸ¥æ¸…å•

#### iOSå¼€å‘æ¸…å•  
- [ ] æ·»åŠ TensorFlow Lite Podä¾èµ–
- [ ] å°†æ¨¡å‹æ–‡ä»¶æ·»åŠ åˆ°Bundle
- [ ] é…ç½®ç›¸æœºä½¿ç”¨æƒé™æè¿°
- [ ] å®ç°EmotionClassifierç±»
- [ ] è®¾ç½®AVCaptureSession
- [ ] å®ç°å›¾åƒé¢„å¤„ç†ï¼ˆUIImageè½¬æ¢ï¼‰
- [ ] æ·»åŠ å®æ—¶ç»“æœæ˜¾ç¤º
- [ ] å¤„ç†æƒé™è¯·æ±‚
- [ ] æµ‹è¯•ä¸åŒiOSç‰ˆæœ¬å…¼å®¹æ€§
- [ ] ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œæ€§èƒ½

---

**ğŸ“ é‡è¦æé†’**: 
1. ä¼˜å…ˆä½¿ç”¨`emotion_model_int8.tflite`è·å¾—æœ€ä½³ç§»åŠ¨ç«¯æ€§èƒ½
2. æ³¨æ„INT8æ¨¡å‹çš„æ•°æ®é¢„å¤„ç†å’Œåå¤„ç†
3. å®ç°å¸§ç‡é™åˆ¶é¿å…è¿‡åº¦æ¶ˆè€—ç”µæ± 
4. æ·»åŠ é€‚å½“çš„é”™è¯¯å¤„ç†å’Œç”¨æˆ·åé¦ˆ
5. åœ¨çœŸå®è®¾å¤‡ä¸Šå……åˆ†æµ‹è¯•æ€§èƒ½è¡¨ç° 